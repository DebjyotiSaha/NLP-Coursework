{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "athletic-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "encouraging-mission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\hp\\.conda\\envs\\tensorflow\\lib\\site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wooden-north",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glove.6B.100d.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url= 'https://raw.githubusercontent.com/clinc/oos-eval/master/data/data_full.json'\n",
    "wget.download(url)\n",
    "\n",
    "url= 'https://www.dropbox.com/s/a247ju2qsczh0be/glove.6B.100d.txt?dl=1'\n",
    "wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tested-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "surface-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_full.json') as file:\n",
    "    data=json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "harmful-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ues=np.array(data['oos_val'])\n",
    "train=np.array(data['oos_train'])\n",
    "test=np.array(data['oos_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "million-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_others=np.array(data['val'])\n",
    "train_o=np.array(data['train'])\n",
    "test_o=np.array(data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "intended-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "val=np.concatenate([val_ues, val_others])\n",
    "train=np.concatenate([train, train_o])\n",
    "test=np.concatenate([test, test_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indonesian-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.concatenate([train, test, val])\n",
    "data=data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "understanding-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=data[0]\n",
    "labels=data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "legitimate-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "floppy-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt,test_txt,train_label,test_labels = train_test_split(text,labels,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "legendary-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "single-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=[]\n",
    "for c in train_txt:\n",
    "    ls.append(len(c.split()))\n",
    "maxLen=int(np.percentile(ls, 98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "typical-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index={}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        coefs=np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word]=coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "gorgeous-jumping",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3357: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.004451992, 0.4081574)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs=np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std= all_embs.mean(), all_embs.std()\n",
    "emb_mean, emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "spoken-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_words=40000\n",
    "embedding_dim=len(embeddings_index['the'])\n",
    "classes=np.unique(labels)\n",
    "\n",
    "tokenizer=Tokenizer(num_words=max_num_words)\n",
    "tokenizer.fit_on_texts(train_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mental-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences=tokenizer.texts_to_sequences(train_txt)\n",
    "train_sequences=pad_sequences(train_sequences, maxlen=maxLen, padding='post')\n",
    "test_sequences=tokenizer.texts_to_sequences(test_txt)\n",
    "test_sequences=pad_sequences(test_sequences, maxlen=maxLen, padding='post')\n",
    "word_index=tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mediterranean-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=min(max_num_words, len(word_index))+1\n",
    "embedding_matrix=np.random.normal(emb_mean, emb_std, (num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_num_words:\n",
    "        break\n",
    "    embedding_vector=embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "later-auditor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(sparse=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(classes)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoder.fit(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "working-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_encoded = label_encoder.transform(train_label)\n",
    "train_label_encoded = train_label_encoded.reshape(len(train_label_encoded), 1)\n",
    "train_label = onehot_encoder.transform(train_label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "decimal-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "test_labels_encoded = test_labels_encoded.reshape(len(test_labels_encoded), 1)\n",
    "test_labels = onehot_encoder.transform(test_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "temporal-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "incredible-archive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 16, 100)           633700    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 16, 512)           731136    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                12850     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 151)               7701      \n",
      "=================================================================\n",
      "Total params: 2,172,843\n",
      "Trainable params: 1,539,143\n",
      "Non-trainable params: 633,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words, 100, trainable=False,input_length=train_sequences.shape[1], weights=[embedding_matrix]))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True, recurrent_dropout=0.1, dropout=0.1), 'concat'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(256, return_sequences=False, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(classes.shape[0], activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "early-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "appointed-coach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "260/260 [==============================] - 136s 448ms/step - loss: 4.6117 - acc: 0.0510 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "260/260 [==============================] - 116s 448ms/step - loss: 3.1774 - acc: 0.1909 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "260/260 [==============================] - 117s 448ms/step - loss: 2.1305 - acc: 0.4042 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "260/260 [==============================] - 116s 446ms/step - loss: 1.5654 - acc: 0.5510 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "260/260 [==============================] - 117s 450ms/step - loss: 1.2126 - acc: 0.6530 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "260/260 [==============================] - 113s 436ms/step - loss: 0.9786 - acc: 0.7174 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "260/260 [==============================] - 113s 436ms/step - loss: 0.8218 - acc: 0.7659 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "260/260 [==============================] - 114s 439ms/step - loss: 0.7134 - acc: 0.7968 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "260/260 [==============================] - 114s 439ms/step - loss: 0.6058 - acc: 0.8306 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "260/260 [==============================] - 119s 459ms/step - loss: 0.5475 - acc: 0.8478 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "260/260 [==============================] - 120s 463ms/step - loss: 0.4796 - acc: 0.8667 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "260/260 [==============================] - 110s 422ms/step - loss: 0.4365 - acc: 0.8779 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "260/260 [==============================] - 116s 446ms/step - loss: 0.3749 - acc: 0.8980 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "260/260 [==============================] - 117s 449ms/step - loss: 0.3502 - acc: 0.9028 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "260/260 [==============================] - 118s 455ms/step - loss: 0.3283 - acc: 0.9079 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "260/260 [==============================] - 119s 457ms/step - loss: 0.2845 - acc: 0.9212 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "260/260 [==============================] - 119s 457ms/step - loss: 0.2765 - acc: 0.9257 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "260/260 [==============================] - 117s 450ms/step - loss: 0.2497 - acc: 0.9328 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "260/260 [==============================] - 117s 449ms/step - loss: 0.2547 - acc: 0.9317 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "260/260 [==============================] - 118s 453ms/step - loss: 0.2231 - acc: 0.9410 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_sequences, train_label, epochs = 20,\n",
    "          batch_size = 64, shuffle=True,\n",
    "          validation_data=[test_sequences, test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "urban-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "smoking-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/intents.h5')\n",
    "\n",
    "with open('utils/classes.pkl', 'wb') as file:\n",
    "    pickle.dump(classes, file)\n",
    "\n",
    "with open('utils/tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "with open('utils/label_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(label_encoder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "relevant-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassifier:\n",
    "    def __init__(self, classes, model, tokenizer, label_encoder):\n",
    "        self.classes=classes\n",
    "        self.classifier=model\n",
    "        self.tokenizer=tokenizer\n",
    "        self.label_encoder=label_encoder\n",
    "    \n",
    "    def get_intent(self, text):\n",
    "        self.text=[text]\n",
    "        self.test_keras=self.tokenizer.texts_to_sequences(self.text)\n",
    "        self.test_keras_sequence=pad_sequences(self.test_keras, maxlen=16, padding='post')\n",
    "        self.pred=self.classifier.predict(self.test_keras_sequence)\n",
    "        return label_encoder.inverse_transform(np.argmax(self.pred, 1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "jewish-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "model=load_model('models/intents.h5')\n",
    "\n",
    "with open('utils/classes.pkl', 'rb') as file:\n",
    "    classes=pickle.load(file)\n",
    "\n",
    "with open('utils/tokenizer.pkl', 'rb') as file:\n",
    "    tokenizer=pickle.load(file)\n",
    "\n",
    "with open('utils/label_encoder.pkl', 'rb') as file:\n",
    "    label_encoder=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "mighty-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu=IntentClassifier(classes, model, tokenizer, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "perceived-amsterdam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlu.get_intent(\"is it cold in India right now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-moldova",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

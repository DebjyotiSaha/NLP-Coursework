{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60f88ba-6209-44c3-b131-ce9cc59f7a73",
   "metadata": {},
   "source": [
    "### NLP building a Chat bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856bff1-051e-4110-b94b-586c348ba92b",
   "metadata": {},
   "source": [
    "We as a group descided to build a chat bot that gives information about the resturants presenets on the premises of the university of surrey \n",
    "\n",
    "We are considering the following restuarnats\n",
    "    \n",
    "    Lakeside\n",
    "    Starbucks\n",
    "    Teas and Tees\n",
    "    Theatre   \n",
    "    Vet school cafe\n",
    "    Pizzaman\n",
    "    Youngs\n",
    "    Hillside\n",
    "    Wates House\n",
    "\n",
    "The main target is to provide information on timings , location and type of the resturant. Our intitial approach is to use intent and NER to understand the request and build simple dialogue flow using those \n",
    "\n",
    "We developed a data set for training the Intent classifier and also built a custom NER model using a blank spacy model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d78d9-eda5-4118-ad07-e1a05f655953",
   "metadata": {},
   "source": [
    "Model serving options info here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff264bf5-218c-498d-b4e0-053b3acf87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_intent(data,train_flag):\n",
    "    #Importing the libraries for intent training\n",
    "    import wget\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pickle\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "    #if loop to verify whether the \".txt\" file is present or not, to satisfy the CI/CD pipeline\n",
    "    if (not (os.path.exists('./glove.6B.100d.txt'))):\n",
    "        print(\"inesle \")\n",
    "        url= 'https://www.dropbox.com/s/a247ju2qsczh0be/glove.6B.100d.txt?dl=1'\n",
    "        wget.download(url)\n",
    "    #getting the json file keys from the data \n",
    "    keyss = list(data.keys())\n",
    "    array11=np.array(data[keyss[0]])\n",
    "    array12=np.array(data[keyss[1]])\n",
    "    array13=np.array(data[keyss[2]])\n",
    "    array14=np.array(data[keyss[3]])\n",
    "    array15=np.array(data[keyss[4]])\n",
    "    #Merging all the keys into one variable for training and testing\n",
    "    val=np.concatenate([array11, array12,array13,array14,array15])\n",
    "    text=[]\n",
    "    labels=[]\n",
    "    for i in  range(len(val)):\n",
    "        text.append(val[i][0])\n",
    "        labels.append(val[i][1])\n",
    "    train_txt,test_txt,train_label,test_labels = train_test_split(text,labels,test_size = 0.2)\n",
    "    #Implementing bag of words to collect all the words\n",
    "    ls=[]\n",
    "    for c in train_txt:\n",
    "        ls.append(len(c.split()))\n",
    "    maxLen=int(np.percentile(ls, 98))\n",
    "    embeddings_index={}\n",
    "    with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values=line.split()\n",
    "            word=values[0]\n",
    "            coefs=np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word]=coefs\n",
    "    all_embs=np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std= all_embs.mean(), all_embs.std()\n",
    "    max_num_words=40000\n",
    "    embedding_dim=len(embeddings_index['the'])\n",
    "    classes=np.unique(labels)\n",
    "    tokenizer=Tokenizer(num_words=max_num_words)\n",
    "    tokenizer.fit_on_texts(train_txt)\n",
    "    train_sequences=tokenizer.texts_to_sequences(train_txt)\n",
    "    train_sequences=pad_sequences(train_sequences, maxlen=maxLen, padding='post')\n",
    "    test_sequences=tokenizer.texts_to_sequences(test_txt)\n",
    "    test_sequences=pad_sequences(test_sequences, maxlen=maxLen, padding='post')\n",
    "    word_index=tokenizer.word_index\n",
    "    num_words=min(max_num_words, len(word_index))+1\n",
    "    embedding_matrix=np.random.normal(emb_mean, emb_std, (num_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_num_words:\n",
    "            break\n",
    "        embedding_vector=embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "    #Initialising One Hot Encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(classes)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoder.fit(integer_encoded)\n",
    "    train_label_encoded = label_encoder.transform(train_label)\n",
    "    train_label_encoded = train_label_encoded.reshape(len(train_label_encoded), 1)\n",
    "    train_label = onehot_encoder.transform(train_label_encoded)\n",
    "    test_labels_encoded = label_encoder.transform(test_labels)\n",
    "    test_labels_encoded = test_labels_encoded.reshape(len(test_labels_encoded), 1)\n",
    "    test_labels = onehot_encoder.transform(test_labels_encoded)\n",
    "    #Dumping the pickle file\n",
    "    with open('utils/tokenizer.pkl', 'wb') as file:\n",
    "        pickle.dump(tokenizer, file)\n",
    "    with open('utils/label_encoder.pkl', 'wb') as file:\n",
    "        pickle.dump(label_encoder, file)\n",
    "    with open('utils/classes.pkl', 'wb') as file:\n",
    "        pickle.dump(classes, file)\n",
    "    if train_flag == 1:\n",
    "        intent_train_model(num_words,train_sequences,train_label,test_sequences,test_labels,embedding_matrix,classes)\n",
    "    return(\"Intent training done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00965ec-37cf-4829-8c66-e2864a4b1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_train_model(num_words,train_sequences,train_label,test_sequences,test_labels,embedding_matrix,classes):\n",
    "    #print(\"in training\")\n",
    "    #Importing libraries for feed forward neural network\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional,Embedding\n",
    "    import pickle\n",
    "    import json\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 100, trainable=False,input_length=train_sequences.shape[1], weights=[embedding_matrix]))\n",
    "    model.add(Bidirectional(LSTM(256, return_sequences=True, recurrent_dropout=0.1, dropout=0.1), 'concat'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(256, return_sequences=False, recurrent_dropout=0.1, dropout=0.1))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(classes.shape[0], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    history = model.fit(train_sequences, train_label, epochs = 15,\n",
    "              batch_size = 20, shuffle=True,\n",
    "              validation_data=[test_sequences, test_labels])\n",
    "    model.save('models/intents.h5')\n",
    "    return(\"Intent training done\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e345d29-f1c9-48c8-8b4e-677fea6a21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner(f): \n",
    "    import spacy\n",
    "    from spacy.tokens import DocBin\n",
    "    from tqdm import tqdm\n",
    "    import json\n",
    "    import os \n",
    "    if (os.path.exists('./model-best')):\n",
    "        nlp = spacy.load(\"model-best\") # if already a trained model exists build upon it \n",
    "    else :\n",
    "        nlp = spacy.blank(\"en\") # if not load a blank spacy model\n",
    "    db = DocBin()\n",
    "\n",
    "    TRAIN_DATA = json.load(f)\n",
    "\n",
    "    for text, annot in tqdm(TRAIN_DATA['annotations']): \n",
    "        doc = nlp.make_doc(text) \n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents \n",
    "        db.add(doc)\n",
    "\n",
    "    db.to_disk(\"./training_data.spacy\") # save the docbin object\n",
    "\n",
    "    ! python -m spacy init config config.cfg --lang en --pipeline ner --optimize efficiency\n",
    "\n",
    "    ! python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy\n",
    "    \n",
    "    return(\"Training done and New ner is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9257721-a449-4266-b6ec-2fb292c60e97",
   "metadata": {},
   "source": [
    "This following can be used for CI CD as well we can run and retrain models once we collect enough data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9c213-ddba-4546-be2f-40bbf5952665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training all the models and save them \n",
    "\n",
    "import json\n",
    "with open('CW11.json') as file:\n",
    "    data=json.loads(file.read())\n",
    "train_flag=1\n",
    "print(train_intent(data,train_flag))#training intent file \n",
    "f = open('version3.json')\n",
    "print(train_ner(f)) # traingin ner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2caeb4af-cfab-4a0c-bcf7-94453628c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ner_function():\n",
    "    def load_ner():\n",
    "        import spacy\n",
    "        from spacy.tokens import DocBin\n",
    "        from tqdm import tqdm\n",
    "        global nlp_ner\n",
    "        nlp_ner = spacy.load(\"model-best\")\n",
    "    def get_ner(sent,tag):\n",
    "        flag =0\n",
    "        doc = nlp_ner(sent) \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == tag:\n",
    "                return (ent.text)\n",
    "                flag =1\n",
    "        if flag == 0:\n",
    "            return(\"NO_ENTITY_FOUND\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71040b0a-e76f-4c26-b9d2-d1d1fa391aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class intent_function():\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    def load_intent():\n",
    "        import pickle\n",
    "        import numpy as np\n",
    "        from keras.models import load_model\n",
    "        from keras.preprocessing.sequence import pad_sequences\n",
    "        global model\n",
    "        model=load_model('models/intents.h5')\n",
    "        with open('utils/classes.pkl', 'rb') as file:\n",
    "            global classes\n",
    "            classes=pickle.load(file)\n",
    "        with open('utils/tokenizer.pkl', 'rb') as file:\n",
    "            global tokenizer\n",
    "            tokenizer=pickle.load(file)\n",
    "        with open('utils/label_encoder.pkl', 'rb') as file:\n",
    "            global label_encoder\n",
    "            label_encoder=pickle.load(file)\n",
    "    def get_intent(senttt):\n",
    "        from keras.preprocessing.sequence import pad_sequences\n",
    "        import numpy as np\n",
    "        class IntentClassifier:\n",
    "            from keras.preprocessing.sequence import pad_sequences\n",
    "            def __init__(self, classes, model, tokenizer, label_encoder):\n",
    "                self.classes=classes\n",
    "                self.classifier=model\n",
    "                self.tokenizer=tokenizer\n",
    "                self.label_encoder=label_encoder\n",
    "            def predict_intent(self, text):\n",
    "                from keras.preprocessing.sequence import pad_sequences\n",
    "                self.text=[text]\n",
    "                self.test_keras=self.tokenizer.texts_to_sequences(self.text)\n",
    "                self.test_keras_sequence=pad_sequences(self.test_keras, maxlen=8, padding='post')\n",
    "                self.pred=self.classifier.predict(self.test_keras_sequence)\n",
    "                return label_encoder.inverse_transform(np.argmax(self.pred, 1))[0]\n",
    "        nlu=IntentClassifier(classes, model, tokenizer, label_encoder)\n",
    "        return(nlu.predict_intent(senttt))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240d17ed-3899-4c01-8d40-694696c08f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_question(senttt):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    question_words = [\"what\", \"why\", \"when\", \"where\", \n",
    "             \"name\", \"is\", \"how\", \"do\", \"does\", \n",
    "             \"which\", \"are\", \"could\", \"would\", \n",
    "             \"should\", \"has\", \"have\", \"whom\", \"whose\", \"don't\"]\n",
    "\n",
    "    question = senttt\n",
    "    question = question.lower()\n",
    "    question = word_tokenize(question)\n",
    "\n",
    "    if any(x in question[0] for x in question_words):\n",
    "        return (1)\n",
    "    else:\n",
    "        return (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c85aef0c-4355-4c3e-9bce-36e65aacda36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(rest_name,info_needed):\n",
    "    import ast\n",
    "    file = open(\"dictionary.txt\", \"r\")\n",
    "    contents = file.read()\n",
    "    dictionary = ast.literal_eval(contents)\n",
    "    information = dictionary[rest_name][info_needed]\n",
    "    return information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "386fbeb3-ab3e-4328-aa2d-7ddbd869f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(intent_detected,sentence):\n",
    "    import random \n",
    "    resp_greet =['Hello', 'Heya','Hi','Hi! How are you doing?','Hey!how are you doing?','Hola','Hello! hope you are doing well','Hello! Whats up?','Hie! how have you been?','Hie!']\n",
    "    resp_goobye =['Bye Bye!', 'Goodbye','See you!','Bye, it was nice talking to you','Goodbye!, have a nice day','I hope this works','thankyou for having a chat','thank you! I hope this chat helped you']\n",
    "    resp_timings =['time','timeess']\n",
    "    resp_type=['typeeee','typessss']\n",
    "    resp_locc =['locccc','location']\n",
    "    sorry = \"Sorry i dont understand the question i will prepare well next time\"\n",
    "    if intent_detected == 'greeting':\n",
    "        return (random.choice(resp_greet))\n",
    "    elif intent_detected == 'goodbye':\n",
    "        return (random.choice(resp_goobye))\n",
    "    elif intent_detected == 'info_timings':\n",
    "        namess = ner_function.get_ner(sentence,'RESTAURANT_NAME')\n",
    "        if namess == \"NO_ENTITY_FOUND\" :\n",
    "            return (sorry)\n",
    "        elif 'open' in sentence:\n",
    "            ress = namess + ' opening timings are ' + get_info(namess,'info_open')\n",
    "            return (ress)\n",
    "        elif 'close' in sentence:\n",
    "            ress = namess + ' Closing timings are ' + get_info(namess,'info_close')\n",
    "            return (ress)\n",
    "    elif intent_detected == 'info_type':\n",
    "        namess = ner_function.get_ner(sentence,'RESTAURANT_NAME')\n",
    "        if namess == \"NO_ENTITY_FOUND\":\n",
    "            return (sorry)\n",
    "        else :\n",
    "            ress = namess + ' is  ' + get_info(namess,'type')\n",
    "            return (ress)     \n",
    "    elif intent_detected == 'info_location':\n",
    "        namess = ner_function.get_ner(sentence,'RESTAURANT_NAME')\n",
    "        if namess == \"NO_ENTITY_FOUND\":\n",
    "            return (sorry)\n",
    "        else :          \n",
    "            ress = namess + ' is located  ' + get_info(namess,'location')\n",
    "            return (ress)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f5aa755-6791-4790-9313-cd5d11f7da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_logs(sent_list,resp_list,intent_list):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    df = pd.DataFrame(list(zip(sent_list, resp_list,intent_list)),columns =['input', 'response','intent'])\n",
    "    if (not (os.path.exists('./logs.csv'))):\n",
    "        df.to_csv(\"logs.csv\")\n",
    "    else :\n",
    "        df1 = pd.read_csv('logs.csv')\n",
    "        df = pd.concat([df, df1])\n",
    "        df.to_csv(\"logs.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461d723e-130d-4266-b80b-e14986d2c021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi i am INFORMIA and i will help you find information about restuarants in Univertsity of surrey\n",
      "Disclaimer :: ---- I am still learning and will get better over time please dont be MAD AT ME\n",
      "Let's chat! (type 'quit' to exit)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infomania: Hola\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  where is hillside\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infomania: hillside is located  stag hill\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  why ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infomania: Heya\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  good byr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infomania: Hi\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "bot_name ='infomania'\n",
    "intent_function.load_intent()\n",
    "ner_function.load_ner()\n",
    "print(\"Hi i am INFORMIA and i will help you find information about restuarants in Univertsity of surrey\")\n",
    "print(\"Disclaimer :: ---- I am still learning and will get better over time please dont be MAD AT ME\") \n",
    "print(\"Let's chat! (type 'quit' to exit)\")\n",
    "bot_name ='infomania'\n",
    "sent_list=[]\n",
    "resp_list=[]\n",
    "intent_list =[]\n",
    "curr_log=[]\n",
    "while True:\n",
    "    # sentence = \"do you use credit cards?\"\n",
    "    sentence = input(\"You: \")\n",
    "    curr_log.append(('You :' + sentence))\n",
    "    sent_list.append(sentence)\n",
    "    if sentence == \"quit\":\n",
    "        store_logs(sent_list,resp_list,intent_list)\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        intent_detected = intent_function.get_intent(sentence)\n",
    "        intent_list.append(intent_detected)\n",
    "        response = get_response(intent_detected,sentence)\n",
    "        resp_list.append(response)\n",
    "        print(f\"{bot_name}:\",response )\n",
    "        curr_log.append(('infomania :' + response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d74a91e-b841-40d5-a8ee-6295b09b1dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You :hello', 'infomania :Hola', 'You :where is hillside', 'infomania :hillside is located  stag hill', 'You :why ?', 'infomania :Heya', 'You :good byr', 'infomania :Hi', 'You :quit']\n"
     ]
    }
   ],
   "source": [
    "print(curr_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0e797-c867-4457-835a-4406d38b33de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
